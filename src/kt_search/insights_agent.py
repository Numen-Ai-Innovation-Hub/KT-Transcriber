"""
Insights Agent - Agente que extrai insights diretos baseados nos resultados da busca sem√¢ntica
Analisa contextos encontrados e gera insights objetivos para responder perguntas
"""

import re
import time
from dataclasses import dataclass
from typing import Any

from openai import OpenAI

from utils.logger_setup import LoggerManager

from .insight_processors import InsightProcessors
from .query_type_detector import QueryTypeDetector

logger = LoggerManager.get_logger(__name__)


@dataclass
class DirectInsightResult:
    """Resultado da an√°lise do agente de insights"""

    insight: str
    confidence: float
    sources_used: int
    processing_time: float
    fallback_used: bool = False


class InsightsAgent:
    """
    Agente especializado em extrair insights diretos baseados em resultados de busca

    Funcionalidade:
    - Analisa m√∫ltiplos contextos encontrados na busca sem√¢ntica
    - Extrai insights diretos e objetivos para responder perguntas
    - Consolida informa√ß√µes de diferentes fontes
    - Prioriza informa√ß√µes por relev√¢ncia e import√¢ncia
    - Gera percep√ß√µes acion√°veis sobre os dados encontrados
    """

    def __init__(self, openai_client: OpenAI | None = None):
        """
        Inicializa o agente de insights

        Args:
            openai_client: Cliente OpenAI para gera√ß√£o de insights
        """
        if openai_client is None:
            from src.config.settings import OPENAI_API_KEY

            openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
        self.openai_client = openai_client
        self.model = "gpt-4o-mini"

        # Cache simples para insights (para otimiza√ß√£o de performance)
        self._insights_cache: dict[str, Any] = {}
        self._cache_max_size = 100

        # Templates de prompts especializados para extra√ß√£o de insights
        from .insights_prompts import PROMPT_TEMPLATES

        self.prompt_templates = PROMPT_TEMPLATES
        self._query_type_detector = QueryTypeDetector()
        self._processors = InsightProcessors()

        logger.info(f"InsightsAgent inicializado com modelo {self.model}")

    def generate_direct_insight(
        self, original_query: str, search_results: list[Any], query_type: str | None = None
    ) -> DirectInsightResult | None:
        """
        Extrai insights diretos baseados nos resultados da busca

        Args:
            original_query: Pergunta original do usu√°rio
            search_results: Lista de resultados da busca sem√¢ntica

        Returns:
            DirectInsightResult com insight gerado ou None se falhou
        """
        # Cache de insights para performance
        from utils.hash_manager import get_hash_manager

        cache_key = get_hash_manager().generate_content_hash(f"{original_query}_{len(search_results)}")
        if cache_key in self._insights_cache:
            logger.info("Insight encontrado no cache - retornando imediatamente")
            return self._insights_cache[cache_key]

        # üöÄ CORRE√á√ÉO CR√çTICA: Smart Response Templates para discrep√¢ncias
        cross_client_warning = self._detect_cross_client_warning(search_results)
        if cross_client_warning:
            logger.warning(f"‚ö†Ô∏è Cross-client discrepancy detected: {cross_client_warning}")
            return self._generate_cross_client_response(original_query, search_results, cross_client_warning)

        if not self.openai_client:
            logger.warning("OpenAI client n√£o dispon√≠vel - usando fallback")
            return self._generate_fallback_insight(original_query, search_results)

        if not search_results:
            logger.warning("Nenhum resultado fornecido para an√°lise")
            result = DirectInsightResult(
                insight="N√£o foram encontrados contextos relevantes para gerar insights sobre sua consulta.",
                confidence=0.0,
                sources_used=0,
                processing_time=0.0,
                fallback_used=True,
            )

        # Verificar qualidade m√≠nima dos resultados
        if len(search_results) < 2:
            logger.warning("Poucos resultados para an√°lise robusta - usando fallback")
            return self._generate_fallback_insight(original_query, search_results)

        try:
            start_time = time.time()
            suffix = "..." if len(original_query) > 100 else ""
            logger.info(f"Iniciando extra√ß√£o de insights para: '{original_query[:100]}{suffix}'")

            # 1. Analisar relev√¢ncia contextual dos resultados
            context_analysis = self._processors.analyze_context_relevance(original_query, search_results)
            logger.info(f"An√°lise contextual: {context_analysis['primary_theme']}")

            # 2. Preparar contextos para an√°lise com foco na relev√¢ncia (incluir query original para filtro sem√¢ntico)
            context_analysis["original_query"] = original_query
            formatted_contexts = self._processors.format_contexts_for_llm(search_results, context_analysis)
            logger.info("Formatados contextos para an√°lise LLM (com filtro sem√¢ntico)")

            # 3. Detectar tipo de consulta para usar prompt especializado
            query_type = self._detect_query_type(original_query, search_results)
            logger.info(f"üéØ InsightsAgent - Tipo de consulta detectado: {query_type} para query: '{original_query}'")

            # üöÄ FASE 3 OTIMIZA√á√ÉO: Fast-track para queries de listagem de KTs
            if query_type == "metadata_listing":
                logger.info("‚ö° FAST-TRACK: Query de metadata listing - gerando resposta com agrupamento por KTs")
                return self._generate_fast_metadata_response(original_query, search_results, start_time)

            # üö® P1-1 FIX: Handler para cliente inexistente
            if query_type == "client_not_found":
                logger.info("üö´ FAST-TRACK: Cliente inexistente - gerando resposta de n√£o encontrado")
                return self._generate_client_not_found_response(original_query, start_time)

            # 4. Gerar prompt especializado com an√°lise contextual
            prompt = self._build_specialized_prompt(query_type, original_query, formatted_contexts, context_analysis)

            # 5. Configura√ß√£o adaptativa baseada no tipo de query
            performance_config = self._processors.get_performance_config(query_type, len(search_results))

            # 6. Chamar OpenAI para an√°lise e extra√ß√£o de insights
            logger.info(
                f"Enviando contextos para extra√ß√£o de insights OpenAI (config: {performance_config['strategy']})"
            )
            response = self.openai_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": f"Analise reuni√µes corporativas e extraia insights objetivos.\n\n{prompt}",
                    }
                ],
                temperature=performance_config["temperature"],
                top_p=performance_config["top_p"],
                presence_penalty=0.0,
                frequency_penalty=0.0,
                max_tokens=performance_config["max_tokens"],
                timeout=performance_config["timeout"],
            )

            # 6. Extrair e processar insight
            raw_insight = (response.choices[0].message.content or "").strip()
            direct_insight = self._processors.format_insight_text(raw_insight)
            processing_time = time.time() - start_time

            # 7. Calcular confian√ßa baseada na qualidade dos contextos
            confidence = self._processors.calculate_confidence(search_results, direct_insight)

            logger.info(
                f"Insight direto gerado em {processing_time:.2f}s - "
                f"Confian√ßa: {confidence:.1%}, Tamanho: {len(direct_insight)} chars"
            )

            result = DirectInsightResult(
                insight=direct_insight,
                confidence=confidence,
                sources_used=len(search_results),
                processing_time=processing_time,
                fallback_used=False,
            )

            # Adicionar ao cache
            if len(self._insights_cache) >= self._cache_max_size:
                # Remove o mais antigo (FIFO simples)
                oldest_key = next(iter(self._insights_cache))
                del self._insights_cache[oldest_key]
            self._insights_cache[cache_key] = result

            return result

        except Exception as e:
            logger.error(f"Erro ao gerar insight direto via OpenAI: {e}")
            # Fallback para insight estruturado
            return self._generate_fallback_insight(original_query, search_results)

    def _detect_query_type(self, query: str, results: list[Any]) -> str:
        """
        Detecta o tipo de consulta para usar prompt especializado

        Args:
            query: Pergunta original
            results: Resultados da busca

        Returns:
            Tipo detectado: 'decision', 'problem', 'metadata_listing', 'participants', 'general'
        """
        query_lower = query.lower()

        # üÜï Detectar consultas de principais pontos/highlights
        highlights_keywords = [
            "principais pontos",
            "pontos importantes",
            "resumo da reuni√£o",
            "highlights",
            "pontos-chave",
            "t√≥picos principais",
        ]
        if any(keyword in query_lower for keyword in highlights_keywords):
            return "highlights_summary"

        # Detectar consultas espec√≠ficas sobre projetos
        project_keywords = ["quais projetos", "projetos foram", "projetos mencionados", "lista de projetos"]
        if any(keyword in query_lower for keyword in project_keywords):
            return "project_listing"

        # Detectar consultas de metadados/listagem - EXPANDIDO SEM HARDCODING
        metadata_keywords = [
            # Palavras interrogativas
            "quais",
            "que",
            "o que",
            # Verbos de listagem/exibi√ß√£o
            "liste",
            "listar",
            "enumere",
            "enumerar",
            "mostre",
            "mostrar",
            "exiba",
            "exibir",
            "apresente",
            "apresentar",
            # Substantivos relacionados
            "listagem",
            "lista",
            "rela√ß√£o",
            "cat√°logo",
            "invent√°rio",
            "√≠ndice",
            # Objetos espec√≠ficos (gen√©ricos)
            "v√≠deos",
            "videos",
            "kts",
            "reuni√µes",
            "reunioes",
            "meetings",
            "clientes",
            "projetos",
            "arquivos",
            # Express√µes sobre disponibilidade
            "temos",
            "dispon√≠veis",
            "disponivel",
            "existem",
            "possu√≠mos",
            "temos acesso",
            "base",
            "conhecimento",
            # Palavras de informa√ß√£o
            "informa√ß√µes",
            "informacoes",
            "dados",
            "conte√∫do",
            "conteudo",
            "material",
            "nomes",
        ]
        if any(keyword in query_lower for keyword in metadata_keywords):
            # üöÄ SOLU√á√ÉO GEN√âRICA MULTICAMADA: Template detection robusto e escal√°vel

            # CAMADA 1: Manter padr√µes existentes (100% compatibilidade)
            legacy_patterns = [
                "liste" in query_lower
                and ("v√≠deos" in query_lower or "kts" in query_lower or "reuni√µes" in query_lower),
                "mostre" in query_lower and ("v√≠deos" in query_lower or "kts" in query_lower),
                "quais" in query_lower
                and ("kts" in query_lower or "reuni√µes" in query_lower or "v√≠deos" in query_lower),
                "temos" in query_lower and "dispon√≠veis" in query_lower,
                "base" in query_lower and "conhecimento" in query_lower,
            ]

            # CAMADA 2: Pattern matching inteligente com regex flex√≠vel
            flexible_patterns = [
                # Padr√µes para clientes
                r"\b(quais?|que)\s+.*\b(nomes?|clientes?)\b.*\b(temos|kt|informa√ß√µes?)",
                r"\bnomes?\s+.*\bclientes?\b.*\b(kt|informa√ß√µes?)\b",
                r"\btemos\s+.*\bclientes?\b.*\b(base|conhecimento)\b",
                r"\bclientes?\s+.*\b(dispon√≠veis?|temos|base|conhecimento)\b",
                # Padr√µes para listagens gerais
                r"\bliste?\s+.*\b(clientes?|kts?|documentos?|v√≠deos?)\b",
                r"\b(quais?|que)\s+.*\b(kts?|documentos?|v√≠deos?)\s+.*\btemos\b",
                # Padr√µes para base de conhecimento
                r"\b.*\bbase\s+.*\bconhecimento\b.*\bclientes?\b",
                r"\btemos\s+.*\binforma√ß√µes?\b.*\bclientes?\b",
            ]

            # CAMADA 3: An√°lise por pontua√ß√£o de palavras-chave
            listing_score = 0
            action_verbs = ["liste", "listar", "quais", "que", "mostre", "exiba"]
            entities = ["clientes", "cliente", "kts", "kt", "documentos", "v√≠deos"]
            context_words = ["temos", "dispon√≠veis", "base", "conhecimento", "informa√ß√µes", "nomes"]

            words = query_lower.split()
            for verb in action_verbs:
                if any(verb in word for word in words):
                    listing_score += 3

            for entity in entities:
                if any(entity in word for word in words):
                    listing_score += 2

            for context in context_words:
                if any(context in word for word in words):
                    listing_score += 1

            # VALIDA√á√ÉO: Aplicar detec√ß√£o multicamada com threshold mais rigoroso
            is_listing_query = (
                any(legacy_patterns)  # Camada 1: Compatibilidade
                or any(re.search(pattern, query_lower) for pattern in flexible_patterns)  # Camada 2: Regex
                or listing_score >= 8  # Camada 3: Score threshold mais rigoroso (8.0 vs 5.0 anterior)
            )

            # üö® PROBLEMA 3 FIX: Detectar an√°lise espec√≠fica vs listagem gen√©rica
            is_specific_kt_analysis = self._query_type_detector.detect_specific_kt_analysis(query_lower)

            if is_listing_query and not is_specific_kt_analysis:
                # üö® P1-1 FIX: Validar cliente inexistente antes do fast-track
                if "cliente" in query_lower:
                    client_mentioned = self._processors.extract_client_from_query(query_lower)
                    if client_mentioned and not self._client_exists_in_base(client_mentioned):
                        logger.info(f"üö´ Cliente inexistente detectado: {client_mentioned}")
                        logger.info("   üéØ Template alterado: metadata_listing ‚Üí client_not_found")
                        return "client_not_found"

                logger.info(
                    f"üéØ TEMPLATE DETECTION: Query clara de metadata listing detectada: '{query_lower[:50]}...'"
                )
                regex_match = any(re.search(pattern, query_lower) for pattern in flexible_patterns)
                logger.info(f"   üìä Score: {listing_score}, Legacy: {any(legacy_patterns)}, Regex: {regex_match}")
                return "metadata_listing"
            elif is_specific_kt_analysis:
                logger.info("üîç SPECIFIC KT ANALYSIS: Detectada an√°lise espec√≠fica - usando LLM em vez de fast-track")
                logger.info(f"   üìã Query: '{query_lower[:50]}...' requer an√°lise LLM")
                return "general"  # Use LLM analysis instead of fast-track

            # Verificar se resultados s√£o do tipo metadata (suporta ContextualizedResult)
            if results:
                for result in results:
                    # Verificar se √© ContextualizedResult ou SearchResult direto
                    search_result = getattr(result, "original_result", result)
                    content_type = getattr(search_result, "content_type", "")
                    category = getattr(search_result, "category", "")

                    if content_type == "metadata" or category == "metadados":
                        return "metadata_listing"

                # üîß FALLBACK INTELIGENTE: Se n√£o h√° metadata expl√≠cita mas query pede listagem de v√≠deos/KTs
                # e encontramos resultados, assumir que √© metadata listing
                if len(results) >= 5:  # Threshold razo√°vel para listagem
                    video_related = any(
                        term in query_lower for term in ["v√≠deos", "videos", "kts", "reuni√µes", "meetings"]
                    )
                    if video_related:
                        logger.info(
                            f"üîß FALLBACK TEMPLATE: {len(results)} resultados"
                            " + query relacionada a v√≠deos ‚Üí metadata_listing"
                        )
                        return "metadata_listing"

        # Detectar consultas sobre participantes
        participant_keywords = ["quem participou", "participantes", "quem estava", "pessoas que"]
        if any(keyword in query_lower for keyword in participant_keywords):
            return "participants"

        # Detectar consultas sobre decis√µes
        decision_keywords = ["decis√£o", "decidido", "aprovado", "definido", "acordo", "resolu√ß√£o"]
        if any(keyword in query_lower for keyword in decision_keywords):
            return "decision"

        # Detectar consultas sobre problemas
        problem_keywords = ["problema", "erro", "falha", "dificuldade", "issue", "bug", "cr√≠tico"]
        if any(keyword in query_lower for keyword in problem_keywords):
            return "problem"

        # Analisar categorias dos resultados
        if results:
            categories = [getattr(r, "category", "geral") for r in results]
            if categories.count("decisao") > len(categories) * 0.6:
                return "decision"
            if categories.count("problema") > len(categories) * 0.6:
                return "problem"

        return "general"

    def _build_specialized_prompt(
        self, query_type: str, original_query: str, contexts: str, context_analysis: dict[str, Any] | None = None
    ) -> str:
        """
        Constr√≥i prompt especializado baseado no tipo de consulta e an√°lise contextual

        Args:
            query_type: Tipo de consulta detectado
            original_query: Pergunta original
            contexts: Contextos formatados
            context_analysis: An√°lise contextual dos resultados

        Returns:
            Prompt especializado para extra√ß√£o de insights
        """
        template = self.prompt_templates.get(query_type, self.prompt_templates["general"])

        # Adicionar orienta√ß√£o contextual se dispon√≠vel
        contextual_guidance = ""
        if context_analysis and context_analysis.get("dominant_context"):
            dominant = context_analysis["dominant_context"]
            if dominant.get("client"):
                contextual_guidance = (
                    f"\n\nCONTEXTO DOMINANTE: A maioria dos resultados refere-se ao cliente {dominant['client']}."
                )

            if context_analysis.get("primary_theme"):
                theme = context_analysis["primary_theme"]
                if "reuni√£o" in theme:
                    contextual_guidance += (
                        "\nFOCO: Identifique especificamente sobre qual cliente/reuni√£o a pergunta se refere."
                    )
                elif "listagem" in theme:
                    contextual_guidance += "\nFOCO: Liste todos os clientes mencionados nos contextos."

        return template.format(query=original_query, contexts=contexts) + contextual_guidance

    def _generate_fallback_insight(self, query: str, results: list[Any]) -> DirectInsightResult:
        """
        Gera insight estruturado quando OpenAI n√£o est√° dispon√≠vel

        Args:
            query: Pergunta original
            results: Resultados da busca

        Returns:
            DirectInsightResult com insight estruturado
        """
        logger.info("Gerando insight fallback estruturado")

        if not results:
            return DirectInsightResult(
                insight="N√£o foram encontrados contextos relevantes para gerar insights sobre sua consulta.",
                confidence=0.0,
                sources_used=0,
                processing_time=0.0,
                fallback_used=True,
            )

        # Para poucos resultados, ser mais cauteloso na confian√ßa
        if len(results) == 1:
            confidence_modifier = 0.3  # Baixa confian√ßa com apenas 1 resultado
        else:
            confidence_modifier = 0.5  # Confian√ßa moderada com poucos resultados

        # Construir insight estruturado baseado nos resultados
        insight_parts = []
        insight_parts.append(f"**Insights baseados em {len(results)} resultado(s) relevante(s):**")

        # Resumir os pontos principais
        for i, result in enumerate(results[:3], 1):
            # Lidar com ContextualizedResult ou SearchResult
            if hasattr(result, "original_result"):
                # √â ContextualizedResult
                search_result = result.original_result
                content_preview = getattr(
                    result.context_window, "full_context_text", getattr(search_result, "content", "")
                )[:150]
                speaker = getattr(search_result, "speaker", f"Participante_{i}")
                timestamp = getattr(search_result, "timestamp_formatted", getattr(search_result, "timestamp", "00:00"))
            else:
                # √â SearchResult direto
                search_result = result  # Define search_result tamb√©m para o branch else
                content_preview = getattr(result, "main_content", getattr(result, "content", ""))[:150]
                speaker = getattr(result, "speaker", f"Participante_{i}")
                timestamp = getattr(result, "timestamp_formatted", getattr(result, "timestamp", "00:00"))

            # üîß FIX: Verificar se content_preview est√° vazio
            if not content_preview or content_preview.strip() == "":
                # Tentar extrair de outras fontes
                if hasattr(search_result, "text"):
                    content_preview = getattr(search_result, "text", "")[:150]
                elif hasattr(search_result, "main_content"):
                    content_preview = getattr(search_result, "main_content", "")[:150]

                # Se ainda vazio, usar placeholder descritivo
                if not content_preview or content_preview.strip() == "":
                    content_preview = (
                        "[Conte√∫do relevante encontrado - informa√ß√µes dispon√≠veis sobre o t√≥pico consultado]"
                    )

            if len(content_preview) == 150:
                content_preview += "..."

            # üîß FIX: Evitar "Unknown" no speaker
            if speaker in ["Unknown", "", None]:
                speaker = f"Participante_{i}"

            insight_parts.append(f"\n**Insight {i}**: {speaker} ({timestamp}) - {content_preview}")

        if len(results) > 3:
            insight_parts.append(f"\n... e mais {len(results) - 3} insight(s) adicional(is) dispon√≠vel(is).")

        # Adicionar conclus√£o baseada no tipo
        query_lower = query.lower()
        if "decis√£o" in query_lower or "decidido" in query_lower:
            insight_parts.append(
                "\n\n**Conclus√£o**: Os insights indicam decis√µes espec√≠ficas tomadas nas reuni√µes analisadas."
            )
        elif "problema" in query_lower:
            insight_parts.append(
                "\n\n**Conclus√£o**: Os insights revelam problemas identificados e poss√≠veis solu√ß√µes discutidas."
            )
        else:
            insight_parts.append(
                "\n\n**Conclus√£o**: Os insights extra√≠dos fornecem percep√ß√µes valiosas sobre os t√≥picos consultados."
            )

        fallback_insight = "".join(insight_parts)

        return DirectInsightResult(
            insight=fallback_insight,
            confidence=confidence_modifier,  # Confian√ßa baseada no n√∫mero de resultados
            sources_used=len(results),
            processing_time=0.1,
            fallback_used=True,
        )


    def _detect_cross_client_warning(self, search_results: list[Any]) -> dict | None:
        """
        Detecta se algum resultado cont√©m warning de discrep√¢ncia cliente vs entidade t√©cnica
        """
        for result in search_results:
            # Verificar se √© dict e tem cross_client_warning
            if isinstance(result, dict) and "cross_client_warning" in result:
                return result["cross_client_warning"]

            # Verificar se √© objeto com atributo cross_client_warning
            elif hasattr(result, "cross_client_warning"):
                return getattr(result, "cross_client_warning", None)

        return None

    def _generate_cross_client_response(self, query: str, results: list[Any], warning: dict) -> DirectInsightResult:
        """
        Gera resposta inteligente para discrep√¢ncias cliente vs entidade t√©cnica

        Implementa os Smart Response Templates do handoff
        """
        entity = warning.get("entity", "ENTIDADE")
        requested_client = warning.get("requested_client", "CLIENTE_SOLICITADO")
        found_client = warning.get("found_client", "CLIENTE_ENCONTRADO")

        # Template para entidade encontrada em cliente diferente
        response_template = f"""‚ö†Ô∏è **{entity} encontrada em {found_client}, mas voc√™ perguntou sobre {requested_client}**

**üîç SITUA√á√ÉO IDENTIFICADA:**
A entidade t√©cnica **{entity}** foi encontrada na base de conhecimento,
por√©m nos v√≠deos de KT do cliente **{found_client}**, n√£o do **{requested_client}** que voc√™ mencionou.

**üìã INFORMA√á√ïES DISPON√çVEIS SOBRE {entity}:**
"""

        # Extrair informa√ß√µes sobre a entidade dos resultados
        entity_info: list[str] = []
        for result in results:
            content = ""
            if isinstance(result, dict):
                raw = result.get("text", result.get("content", ""))
                content = raw if isinstance(raw, str) else ""
            elif hasattr(result, "content"):
                raw = getattr(result, "content", "")
                content = raw if isinstance(raw, str) else ""

            if isinstance(content, str) and entity.upper() in content.upper():
                # Extrair trecho relevante (primeiro par√°grafo que menciona a entidade)
                paragraphs = content.split("\n")
                for para in paragraphs:
                    if entity.upper() in para.upper():
                        entity_info.append(para.strip())
                        break

        # Adicionar informa√ß√µes encontradas
        if entity_info:
            for i, info in enumerate(entity_info[:3], 1):  # M√°ximo 3 informa√ß√µes
                response_template += f"\n{i}. {info}"
        else:
            response_template += f"\nDetalhes t√©cnicos sobre {entity} identificados nos v√≠deos de {found_client}."

        # Adicionar orienta√ß√£o clara
        response_template += f"""

**üí° RECOMENDA√á√ÉO:**
- As informa√ß√µes sobre **{entity}** est√£o dispon√≠veis nos KTs do **{found_client}**
- Se voc√™ precisa de **{entity}** especificamente para **{requested_client}**, pode n√£o estar documentado ainda
- Considere verificar se **{entity}** se aplica tamb√©m ao contexto **{requested_client}**

**üîó FONTE:** V√≠deos de Knowledge Transfer do cliente {found_client}"""

        return DirectInsightResult(
            insight=response_template,
            confidence=0.85,  # Alta confian√ßa - sabemos exatamente o que aconteceu
            sources_used=len(results),
            processing_time=0.1,  # R√°pida - template direto
            fallback_used=False,
        )

    def _generate_fast_metadata_response(
        self, query: str, search_results: list[Any], start_time: float
    ) -> DirectInsightResult:
        """üöÄ FASE 3: Gera√ß√£o r√°pida de resposta para queries de listagem de KTs √∫nicos"""

        # Extrair informa√ß√µes de KTs √∫nicos dos metadados
        unique_kts = {}  # {video_name: {client_name, original_url, meeting_date}}
        client_filter = None

        # Detectar filtro de cliente na query primeiro
        query_upper = query.upper()
        known_clients = ["DEXCO", "ARCO", "VISSIMO", "V√çSSIMO"]
        for client in known_clients:
            if client in query_upper:
                client_filter = client.replace("V√çSSIMO", "VISSIMO")  # Normalizar
                break

        for result in search_results:
            # Acessar metadata do resultado ChromaDB
            metadata = result.get("metadata", {}) if isinstance(result, dict) else {}

            # Extrair informa√ß√µes essenciais
            video_name = metadata.get("video_name", "")
            client_name = metadata.get("client_name", "")
            original_url = metadata.get("original_url", "")
            meeting_date = metadata.get("meeting_date", "")

            if video_name and video_name not in unique_kts:
                unique_kts[video_name] = {
                    "client_name": client_name,
                    "original_url": original_url,
                    "meeting_date": meeting_date,
                }

        # Gerar resposta formatada com KTs √∫nicos
        if not unique_kts:
            client_suffix = f"para o cliente {client_filter}" if client_filter else ""
            response_text = f"N√£o foram encontrados KTs {client_suffix} na base de conhecimento."
            confidence = 0.60
        else:
            # Organizar KTs por cliente
            kts_by_client: dict[str, list[dict[str, str]]] = {}
            for video_name, info in unique_kts.items():
                client = info["client_name"]
                if client not in kts_by_client:
                    kts_by_client[client] = []
                kts_by_client[client].append(
                    {"video_name": video_name, "url": info["original_url"], "date": info["meeting_date"]}
                )

            # Debug: Log client filter vs available clients
            logger.debug(f"CLIENT FILTER: '{client_filter}' | AVAILABLE CLIENTS: {list(kts_by_client.keys())}")
            logger.debug(f"CLIENT MATCH: {client_filter in kts_by_client if client_filter else 'No filter'}")

            # Construir resposta formatada
            if client_filter and client_filter in kts_by_client:
                # Resposta espec√≠fica para um cliente
                client_kts = kts_by_client[client_filter]
                response_text = f"**KTs DO CLIENTE {client_filter}:**\n\n"

                for _i, kt in enumerate(client_kts, 1):
                    kt_title = kt["video_name"]
                    # Limpar t√≠tulo para exibi√ß√£o
                    if kt_title.startswith("[") and "] " in kt_title:
                        kt_title = kt_title.split("] ", 1)[1]
                    if "Grava√ß√£o de Reuni√£o" in kt_title:
                        kt_title = kt_title.replace("-Grava√ß√£o de Reuni√£o", "").strip()
                        if kt_title.endswith("_150"):
                            kt_title = kt_title.rsplit("_", 1)[0]

                    response_text += f"‚Ä¢ **{kt_title}**\n"
                    if kt["url"]:
                        response_text += f"  Link: {kt['url']}\n"
                    if kt["date"]:
                        response_text += f"  Data: {kt['date']}\n"
                    response_text += "\n"

                confidence = 0.90
            else:
                # Resposta geral para todos os clientes
                response_text = "**KTs REGISTRADOS NA BASE DE CONHECIMENTO:**\n\n"

                for client, client_kts in kts_by_client.items():
                    response_text += f"**{client}:**\n"
                    for kt in client_kts:
                        kt_title = kt["video_name"]
                        # Limpar t√≠tulo para exibi√ß√£o
                        if kt_title.startswith("[") and "] " in kt_title:
                            kt_title = kt_title.split("] ", 1)[1]
                        if "Grava√ß√£o de Reuni√£o" in kt_title:
                            kt_title = kt_title.replace("-Grava√ß√£o de Reuni√£o", "").strip()
                            if kt_title.endswith("_150"):
                                kt_title = kt_title.rsplit("_", 1)[0]

                        response_text += f"  ‚Ä¢ {kt_title}\n"
                        if kt["url"]:
                            response_text += f"    Link: {kt['url']}\n"
                    response_text += "\n"

                confidence = 0.85

            # Adicionar resumo estat√≠stico
            total_kts = len(unique_kts)
            total_clients = len(kts_by_client)
            kts_s = "s" if total_kts != 1 else ""
            clients_s = "s" if total_clients != 1 else ""
            disp_s = "is" if total_kts != 1 else ""
            response_text += (
                f"**Resumo:** {total_kts} KT{kts_s} de {total_clients} cliente{clients_s} dispon√≠vel{disp_s}."
            )

        analysis_time = time.time() - start_time
        logger.info(f"‚ö° Resposta FAST-TRACK gerada em {analysis_time:.3f}s - {len(unique_kts)} KTs √∫nicos")

        return DirectInsightResult(
            insight=response_text,
            confidence=confidence,
            sources_used=len(unique_kts),
            processing_time=analysis_time,
            fallback_used=False,
        )

    def _client_exists_in_base(self, client_name: str) -> bool:
        """Verifica se cliente existe na base de conhecimento consultando diretamente a base"""
        try:
            # Importar DynamicClientManager
            from .dynamic_client_manager import DynamicClientManager

            # Inicializar o client manager se n√£o existir
            if not hasattr(self, "_client_manager"):
                self._client_manager = DynamicClientManager()

            # Descobrir clientes dispon√≠veis na base
            available_clients = self._client_manager.discover_clients()

            # Normalizar nome do cliente
            client_normalized = client_name.upper().strip()

            # Verificar se est√° na lista de clientes descobertos
            for client_info in available_clients.values():
                # Verificar nome principal
                if client_info.name.upper() == client_normalized:
                    return True

                # Verificar varia√ß√µes registradas
                for variation in client_info.variations:
                    if variation.upper() == client_normalized:
                        return True

            logger.info(
                f"üîç Cliente '{client_name}' n√£o encontrado na base. Dispon√≠veis: {list(available_clients.keys())}"
            )
            return False

        except Exception as e:
            logger.warning(f"Erro ao verificar exist√™ncia do cliente {client_name}: {e}")
            return True  # Em caso de erro, assumir que existe para n√£o bloquear

    def _generate_client_not_found_response(self, original_query: str, start_time: float) -> DirectInsightResult:
        """Gera resposta espec√≠fica para cliente inexistente"""
        # Extrair nome do cliente da query
        client_mentioned = self._processors.extract_client_from_query(original_query.lower())

        # Descobrir clientes dispon√≠veis
        try:
            if not hasattr(self, "_client_manager"):
                from .dynamic_client_manager import DynamicClientManager

                self._client_manager = DynamicClientManager()

            available_clients = self._client_manager.discover_clients()
            client_names = list(available_clients.keys())

            response_text = f"**Cliente '{client_mentioned}' n√£o encontrado na base de conhecimento.**\n\n"
            response_text += "**Clientes dispon√≠veis:**\n"
            for client in sorted(client_names):
                response_text += f"‚Ä¢ {client}\n"

            response_text += (
                "\n**Sugest√£o:** Verifique a grafia do nome do cliente ou escolha um dos clientes listados acima."
            )

        except Exception as e:
            logger.warning(f"Erro ao listar clientes dispon√≠veis: {e}")
            response_text = f"**Cliente '{client_mentioned}' n√£o foi encontrado na base de conhecimento.**\n\n"
            response_text += "**Sugest√£o:** Verifique a grafia do nome do cliente e tente novamente."

        analysis_time = time.time() - start_time
        logger.info(f"üö´ Resposta cliente inexistente gerada em {analysis_time:.3f}s para cliente: {client_mentioned}")

        return DirectInsightResult(
            insight=response_text,
            confidence=0.95,  # Alta confian√ßa que cliente n√£o existe
            sources_used=0,
            processing_time=analysis_time,
            fallback_used=False,
        )
