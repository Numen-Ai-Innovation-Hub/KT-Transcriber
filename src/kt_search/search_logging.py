"""
Search Logging - Logger estruturado para as 6 fases do pipeline RAG.

Responsabilidade: emitir logs de debug/info para cada fase do pipeline
SearchEngine sem depender de atributos de instÃ¢ncia do motor de busca.
"""

from typing import Any

from utils.logger_setup import LoggerManager

from .chunk_selector import SelectionResult
from .query_classifier import ClassificationResult
from .query_enricher import EnrichmentResult

logger = LoggerManager.get_logger(__name__)


class PipelineLogger:
    """Logger estruturado para as fases do pipeline RAG."""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 1 â€” Enriquecimento
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_enrichment_phase(
        self,
        original_query: str,
        enrichment_result: EnrichmentResult,
        enrichment_time: float,
        show_details: bool,
    ) -> None:
        """Loga fase de enriquecimento com detalhes ricos."""
        logger.debug(
            f"1ï¸âƒ£ ENRIQUECIMENTO {'DA QUERY ' if show_details else ''}"
            f"| {enrichment_time:.3f}s | conf: {enrichment_result.confidence:.2f}"
        )

        if show_details:
            logger.debug(f'   ğŸ“¥ Query original: "{original_query}" (len: {len(original_query)})')
            logger.debug(f'   ğŸ”„ Query limpa: "{enrichment_result.cleaned_query}"')
            logger.debug(f'   ğŸ“¤ Query enriquecida: "{enrichment_result.enriched_query}"')
            logger.debug(f"   ğŸ“Š ConfianÃ§a do enriquecimento: {enrichment_result.confidence:.3f}")

            logger.debug(f"   ğŸ¯ Entidades detectadas ({len(enrichment_result.entities)} tipos):")
            for entity_type, entity_data in enrichment_result.entities.items():
                if entity_data.get("values"):
                    confidence = entity_data.get("confidence", 0.0)
                    values = entity_data["values"]
                    logger.debug(f"      â€¢ {entity_type}: {values} (conf: {confidence:.3f}, count: {len(values)})")
                else:
                    logger.debug(f"      â€¢ {entity_type}: [] (nÃ£o detectado)")

            context_flags_true = []
            context_flags_values = []
            for key, value in enrichment_result.context.items():
                if isinstance(value, bool) and value:
                    context_flags_true.append(key)
                elif not isinstance(value, bool) and value:
                    context_flags_values.append(f"{key}={value}")

            if context_flags_true:
                logger.debug(f"   ğŸ“‹ Flags contexto: {', '.join(context_flags_true)}")
            if context_flags_values:
                logger.debug(f"   ğŸ“‹ Valores contexto: {', '.join(context_flags_values)}")

            original_words = len(original_query.split())
            enriched_words = len(enrichment_result.enriched_query.split())
            if enriched_words > original_words:
                logger.debug(
                    f"   ğŸ“ˆ ExpansÃ£o: +{enriched_words - original_words} palavras ({original_words} â†’ {enriched_words})"
                )
        else:
            logger.debug(f'   ğŸ“¥ Input: "{original_query}"')
            logger.debug(f'   ğŸ“¤ Output: "{enrichment_result.enriched_query}"')

            entity_summary: dict[str, Any] = {}
            total_entities = 0
            for entity_type, entity_data in enrichment_result.entities.items():
                if entity_data.get("values"):
                    entity_summary[entity_type] = entity_data["values"]
                    total_entities += len(entity_data["values"])

            if entity_summary:
                logger.debug(f"   ğŸ¯ Entidades ({total_entities}): {entity_summary}")
            else:
                logger.debug("   ğŸ¯ Entidades: Nenhuma detectada")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 2 â€” ClassificaÃ§Ã£o
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_classification_phase(
        self,
        classification_result: ClassificationResult,
        classification_time: float,
        show_details: bool,
    ) -> None:
        """Loga fase de classificaÃ§Ã£o com detalhes ricos."""
        logger.debug(
            f"2ï¸âƒ£ CLASSIFICAÃ‡ÃƒO {'DE QUERY ' if show_details else ''}"
            f"| {classification_time:.3f}s | conf: {classification_result.confidence:.2f}"
        )

        if show_details:
            strategy = classification_result.strategy
            logger.debug("   ğŸ“Š Scores de classificaÃ§Ã£o:")
            logger.debug(
                f"      ğŸ¯ {classification_result.query_type.value}:"
                f" {classification_result.confidence:.3f} â­ (ESCOLHIDO)"
            )

            if "debug_scores" in strategy:
                for rag_type, score in strategy["debug_scores"].items():
                    if rag_type != classification_result.query_type.value:
                        logger.debug(f"      â€¢ {rag_type}: {score:.3f}")

            logger.debug(f"   ğŸ“¤ Tipo RAG final: {classification_result.query_type.value}")
            logger.debug(f"   ğŸ¯ Strategy usada: {strategy.get('name', 'default')}")

            if "top_k_modifier" in strategy:
                logger.debug(f"   ğŸ“ TOP_K modifier: {strategy['top_k_modifier']}")
            if "filters" in strategy and strategy["filters"]:
                logger.debug(f"   ğŸ” Filtros da strategy: {strategy['filters']}")

            reasoning = strategy.get("reasoning", "Query classification based on pattern matching")
            logger.debug(f"   ğŸ“‹ RazÃ£o da classificaÃ§Ã£o: {reasoning}")

            detected_features = []
            if "semantic_indicators" in strategy:
                detected_features.append(f"semantic_indicators={strategy['semantic_indicators']}")
            if "metadata_indicators" in strategy:
                detected_features.append(f"metadata_indicators={strategy['metadata_indicators']}")
            if "temporal_indicators" in strategy:
                detected_features.append(f"temporal_indicators={strategy['temporal_indicators']}")

            if detected_features:
                logger.debug(f"   ğŸ§  Features detectadas: {', '.join(detected_features)}")
        else:
            logger.debug(
                f"   ğŸ“¤ Tipo: {classification_result.query_type.value}"
                f" | Strategy: {classification_result.strategy.get('name', 'default')}"
            )
            reasoning = classification_result.strategy.get("reasoning", "Auto-detected")
            logger.debug(f"   ğŸ¯ RazÃ£o: {reasoning}")
            logger.debug(f"   ğŸ“Š ConfianÃ§a: {classification_result.confidence:.3f}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 3 â€” ChromaDB
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_chromadb_phase(
        self,
        raw_results: list[dict[str, Any]],
        chromadb_time: float,
        enrichment_result: EnrichmentResult,
        classification_result: ClassificationResult,
        show_details: bool,
    ) -> None:
        """Loga fase de busca no ChromaDB com detalhes ricos."""
        logger.debug(f"3ï¸âƒ£ BUSCA {'NO ' if show_details else ''}CHROMADB | {chromadb_time:.3f}s")

        query_type = classification_result.query_type.value
        if query_type == "SEMANTIC":
            method = "similarity_search (com embedding)"
            search_detail = "Busca por similaridade semÃ¢ntica usando embeddings"
        elif query_type == "METADATA":
            method = "metadata_search (sem embedding)"
            search_detail = "Busca estruturada apenas em metadados"
        elif query_type == "ENTITY":
            method = "entity_search (metadados + entidades)"
            search_detail = "Busca focada em entidades especÃ­ficas"
        elif query_type == "TEMPORAL":
            method = "temporal_search (com filtros de data)"
            search_detail = "Busca com restriÃ§Ãµes temporais"
        elif query_type == "CONTENT":
            method = "content_search (busca literal)"
            search_detail = "Busca literal no conteÃºdo dos chunks"
        else:
            method = f"{query_type.lower()}_search"
            search_detail = "MÃ©todo de busca personalizado"

        filters_applied: dict[str, str] = {}
        client_normalization_info = None

        if "clients" in enrichment_result.entities:
            client_values = enrichment_result.entities["clients"]["values"]
            if client_values:
                detected_client = client_values[0]
                normalized_client = detected_client.upper()
                filters_applied["client_name"] = normalized_client
                if detected_client.upper() != normalized_client:
                    client_normalization_info = f'"{detected_client}" â†’ "{normalized_client}"'
                else:
                    client_normalization_info = f'"{detected_client}" â†’ "{normalized_client}" (exact match)'

        unique_videos: set[str] = set()
        unique_clients: set[str] = set()
        similarity_scores: list[float] = []

        for result in raw_results:
            metadata = result.get("metadata", {})
            video_name_raw = metadata.get("video_name", "Unknown")
            client_name_raw = metadata.get("client_name", "Unknown")
            video_name = video_name_raw if isinstance(video_name_raw, str) else "Unknown"
            client_name = client_name_raw if isinstance(client_name_raw, str) else "Unknown"
            if video_name != "Unknown":
                unique_videos.add(video_name)
            if client_name != "Unknown":
                unique_clients.add(client_name)
            if "similarity_score" in result:
                similarity_scores.append(result["similarity_score"])

        if show_details:
            logger.debug(f"   ğŸ¯ MÃ©todo de busca: {method}")
            logger.debug(f"   ğŸ“‹ DescriÃ§Ã£o: {search_detail}")

            if query_type == "SEMANTIC":
                logger.debug("   ğŸ§  Embedding gerado para query enriquecida")
                if enrichment_result.enriched_query != enrichment_result.cleaned_query:
                    logger.debug("   ğŸ“ˆ Query expandida usada para embedding")

            if filters_applied:
                logger.debug("   ğŸ“¥ Filtros ChromaDB aplicados:")
                for key, value in filters_applied.items():
                    logger.debug(f'      â€¢ {key}: "{value}"')
                if client_normalization_info:
                    logger.debug(f"   ğŸ”„ NormalizaÃ§Ã£o de cliente: {client_normalization_info}")
            else:
                logger.debug("   ğŸ“¥ Filtros: Nenhum filtro aplicado (busca geral)")

            logger.debug(f"   ğŸ“¤ Resultados brutos: {len(raw_results)} chunks")
            logger.debug("   ğŸ“Š DistribuiÃ§Ã£o:")
            logger.debug(
                f"      â€¢ {len(unique_videos)} vÃ­deos Ãºnicos:"
                f" {', '.join(sorted(unique_videos)[:3])}{'...' if len(unique_videos) > 3 else ''}"
            )
            logger.debug(f"      â€¢ {len(unique_clients)} clientes Ãºnicos: {', '.join(sorted(unique_clients))}")

            if similarity_scores:
                avg_similarity = sum(similarity_scores) / len(similarity_scores)
                max_similarity = max(similarity_scores)
                min_similarity = min(similarity_scores)
                logger.debug(
                    f"   ğŸ“ˆ Similaridade: avg={avg_similarity:.3f}, max={max_similarity:.3f}, min={min_similarity:.3f}"
                )

            strategy = classification_result.strategy
            if "top_k_modifier" in strategy:
                base_limit = strategy.get("base_limit", 20)
                search_limit = int(base_limit * strategy["top_k_modifier"])
                if len(raw_results) >= search_limit:
                    logger.debug(f"   âš ï¸ Limite de busca atingido: {search_limit} chunks (pode haver mais resultados)")
        else:
            filter_display = filters_applied if filters_applied else {"nenhum": "busca_geral"}
            logger.debug(f"   ğŸ“¤ Resultados: {len(raw_results)} chunks | Filtros: {filter_display}")
            logger.debug(f"   ğŸ¯ MÃ©todo: {method}")
            logger.debug(f"   ğŸ“Š Origem: {len(unique_videos)} vÃ­deos, {len(unique_clients)} clientes")
            if client_normalization_info:
                logger.debug(f"   ğŸ”„ Cliente: {client_normalization_info}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 4 â€” Descoberta de clientes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_client_discovery_phase(
        self,
        client_time: float,
        show_details: bool,
        dynamic_client_manager: Any,
    ) -> None:
        """Loga fase de descoberta de clientes.

        Args:
            client_time: Tempo gasto na fase de descoberta.
            show_details: Se True, exibe detalhes completos.
            dynamic_client_manager: InstÃ¢ncia do DynamicClientManager para consulta de clientes.
        """
        logger.debug(f"4ï¸âƒ£ DESCOBERTA {'DE CLIENTES' if show_details else 'CLIENTES'} | {client_time:.3f}s")

        if show_details:
            logger.debug("   ğŸ¯ Processo: Dynamic Client Manager ativo")
            logger.debug("   ğŸ“‹ FunÃ§Ã£o: Enriquecer metadados com descoberta dinÃ¢mica de clientes")

            try:
                clients = dynamic_client_manager.discover_clients()
                client_info = []
                total_chunks = 0

                for client_name, client_data in clients.items():
                    chunk_count = client_data.chunk_count
                    if chunk_count > 0:
                        client_info.append(f"{client_name} ({chunk_count} chunks)")
                        total_chunks += chunk_count

                if client_info:
                    logger.debug(f"   ğŸ” Clientes na base ({len(client_info)} clientes, {total_chunks} chunks total):")
                    for info in client_info:
                        logger.debug(f"      â€¢ {info}")
                else:
                    logger.debug("   ğŸ” Clientes disponÃ­veis: Consultando ChromaDB dynamicamente...")

                logger.debug(
                    f"   ğŸ”„ Cache de clientes: {'HIT' if hasattr(dynamic_client_manager, '_client_cache') else 'MISS'}"
                )

            except Exception as e:
                logger.debug(f"   âš ï¸ Erro ao obter clientes: {str(e)[:50]}...")
                logger.debug("   ğŸ” Fallback: Consultando ChromaDB diretamente...")

            logger.debug("   ğŸ“¤ Resultado: Metadados de contexto client enrichment aplicado")
            _perf = "RÃ¡pido" if client_time < 1.0 else "Normal" if client_time < 3.0 else "Lento"
            logger.debug(f"   âš¡ Performance: {client_time:.3f}s ({_perf})")
        else:
            try:
                clients = dynamic_client_manager.discover_clients()
                client_count = len([c for c, data in clients.items() if data.chunk_count > 0])
                logger.debug(f"   ğŸ” {client_count} clientes na base | Enriquecimento: {client_time:.3f}s")
            except Exception:
                logger.debug(f"   ğŸ¯ Enriquecimento dinÃ¢mico: {client_time:.3f}s")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 5 â€” SeleÃ§Ã£o de chunks
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_selection_phase(
        self,
        selection_result: SelectionResult,
        raw_results: list[dict[str, Any]],
        top_k: int,
        selection_time: float,
        show_details: bool,
    ) -> None:
        """Loga fase de seleÃ§Ã£o de chunks com detalhes ricos."""
        logger.debug(f"5ï¸âƒ£ SELEÃ‡ÃƒO {'INTELIGENTE ' if show_details else 'CHUNKS '}| {selection_time:.3f}s")

        if show_details:
            logger.debug("   ğŸ¯ ChunkSelector: SeleÃ§Ã£o inteligente com Quality + Diversity")
            logger.debug(f"   ğŸ“Š TOP_K adaptativo calculado: {top_k}")
            logger.debug(f"      Base: 20 â†’ Modificado: {top_k} (baseado no tipo de query)")
            logger.debug(f"   ğŸ” EstratÃ©gia selecionada: {selection_result.selection_strategy}")

            strategies = {
                "all_candidates": "Manter todos os candidatos (metadata listing)",
                "quality_filter": "Filtrar por quality threshold",
                "top_k_limit": "Limitar aos TOP_K melhores",
                "diversity_selection": "SeleÃ§Ã£o com diversidade",
            }
            strategy_desc = strategies.get(selection_result.selection_strategy, "EstratÃ©gia personalizada")
            logger.debug(f"   ğŸ“‹ DescriÃ§Ã£o: {strategy_desc}")
            logger.debug(
                f"   ğŸ“¤ Resultado da seleÃ§Ã£o: {len(selection_result.selected_chunks)}/{len(raw_results)} chunks"
            )

            quality_scores = [chunk.get("quality_score", 0.0) for chunk in raw_results if "quality_score" in chunk]
            quality_threshold = 0.3

            if quality_scores:
                avg_quality = sum(quality_scores) / len(quality_scores)
                max_quality = max(quality_scores)
                min_quality = min(quality_scores)
                quality_passed = sum(1 for score in quality_scores if score >= quality_threshold)

                logger.debug("   ğŸ† Quality Analysis:")
                logger.debug(f"      â€¢ Threshold: {quality_threshold}")
                logger.debug(f"      â€¢ Passed threshold: {quality_passed}/{len(quality_scores)} chunks")
                logger.debug(f"      â€¢ Scores: avg={avg_quality:.3f}, max={max_quality:.3f}, min={min_quality:.3f}")
                logger.debug(f"      â€¢ Quality gate: {'âœ… PASSED' if quality_passed > 0 else 'âŒ FAILED'}")

            logger.debug(f"   âœ… Quality threshold met: {selection_result.quality_threshold_met}")

            if selection_result.selection_strategy == "diversity_selection":
                logger.debug("   ğŸŒˆ Diversity: SeleÃ§Ã£o balanceada entre vÃ­deos/clientes")

            chunks_per_second = len(raw_results) / selection_time if selection_time > 0 else float("inf")
            logger.debug(f"   âš¡ Performance: {chunks_per_second:.0f} chunks/s processados")
        else:
            strategy_short = {
                "all_candidates": "Manter todos",
                "quality_filter": "Quality filter",
                "top_k_limit": "TOP_K limit",
                "diversity_selection": "Diversity",
            }.get(selection_result.selection_strategy, selection_result.selection_strategy)

            logger.debug(
                f"   ğŸ“¤ Selecionados: {len(selection_result.selected_chunks)}/{len(raw_results)} | TOP_K: {top_k}"
            )
            logger.debug(
                f"   ğŸ¯ EstratÃ©gia: {strategy_short}"
                f" | Quality: {'âœ…' if selection_result.quality_threshold_met else 'âŒ'}"
            )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fase 6 â€” Insights
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def log_insights_phase(self, insights_result: Any, insights_time: float, show_details: bool) -> None:
        """Loga fase de geraÃ§Ã£o de insights com detalhes ricos."""
        logger.debug(
            f"6ï¸âƒ£ {'GERAÃ‡ÃƒO DE ' if show_details else ''}"
            f"INSIGHTS | {insights_time:.3f}s | conf: {insights_result.confidence:.2f}"
        )

        if show_details:
            logger.debug("   ğŸ¯ InsightsAgent: GeraÃ§Ã£o de resposta inteligente final")

            processing_time = getattr(insights_result, "processing_time", insights_time)
            is_fast_track = processing_time < 0.01

            if is_fast_track:
                template = "metadata_listing (fast-track)"
                method = "Template automÃ¡tico - Sem chamada LLM"
                llm_cost = "R$ 0.00"
            else:
                template = "LLM-generated"
                method = "GPT-4o-mini (texto + contextos)"
                estimated_tokens = len(insights_result.insight) * 1.3
                cost_estimate = estimated_tokens * 0.00001
                llm_cost = f"~R$ {cost_estimate:.4f}"

            logger.debug(f"   ğŸ“‹ Template usado: {template}")
            logger.debug(f"   ğŸ¤– MÃ©todo: {method}")
            logger.debug(f"   ğŸ’° Custo estimado: {llm_cost}")
            logger.debug(f"   âš¡ Fast-track: {'âœ… Ativo' if is_fast_track else 'âŒ Inativo'}")

            if hasattr(insights_result, "unique_videos"):
                logger.debug(f"   ğŸ”„ Agrupamento: Por video_name ({insights_result.unique_videos} vÃ­deos Ãºnicos)")
            else:
                logger.debug("   ğŸ”„ Processamento: AnÃ¡lise semÃ¢ntica completa de contextos")

            confidence_level = insights_result.confidence
            if confidence_level >= 0.9:
                confidence_desc, confidence_emoji = "Muito Alta", "ğŸŸ¢"
            elif confidence_level >= 0.7:
                confidence_desc, confidence_emoji = "Alta", "ğŸŸ¡"
            elif confidence_level >= 0.5:
                confidence_desc, confidence_emoji = "MÃ©dia", "ğŸŸ "
            else:
                confidence_desc, confidence_emoji = "Baixa", "ğŸ”´"

            logger.debug(f"   ğŸ“Š ConfianÃ§a final: {confidence_level:.3f} ({confidence_desc}) {confidence_emoji}")

            response_length = len(insights_result.insight)
            if response_length > 500:
                response_desc = "Resposta detalhada"
            elif response_length > 200:
                response_desc = "Resposta moderada"
            elif response_length > 50:
                response_desc = "Resposta concisa"
            else:
                response_desc = "Resposta muito breve"

            logger.debug(f"   ğŸ“ Resposta: {response_length} chars ({response_desc})")

            if insights_time < 1.0:
                perf_desc, perf_emoji = "Excelente", "ğŸš€"
            elif insights_time < 3.0:
                perf_desc, perf_emoji = "Boa", "âš¡"
            elif insights_time < 5.0:
                perf_desc, perf_emoji = "AceitÃ¡vel", "â±ï¸"
            else:
                perf_desc, perf_emoji = "Lenta", "ğŸŒ"

            logger.debug(f"   âš¡ Performance: {insights_time:.3f}s ({perf_desc}) {perf_emoji}")
        else:
            is_fast_track = insights_time < 0.01
            template = "fast-track" if is_fast_track else "LLM"
            method_desc = "Template" if is_fast_track else "GPT-4o-mini"
            logger.debug(f"   ğŸ“¤ Template: {template} | MÃ©todo: {method_desc}")
            logger.debug(
                f"   ğŸ¯ ConfianÃ§a: {insights_result.confidence:.3f} | {'ğŸš€ Fast' if is_fast_track else 'ğŸ¤– LLM'}"
            )
